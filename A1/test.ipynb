{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Rida\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Rida\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: 'data'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 18\u001b[0m\n\u001b[0;32m     16\u001b[0m filenames \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m#Loop through each file in the directory\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m filename \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m:\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;66;03m#Constructs file path for a specific file in data folder\u001b[39;00m\n\u001b[0;32m     20\u001b[0m     file_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m,filename)\n\u001b[0;32m     21\u001b[0m     filenames\u001b[38;5;241m.\u001b[39mappend(filename)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: 'data'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import nltk\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#Download nltk stuff\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "#Set up stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "preprocessed_directory = 'preprocessed_data'\n",
    "all_words = set()\n",
    "filenames = []\n",
    "#Loop through each file in the directory\n",
    "for filename in os.listdir('data'):\n",
    "    #Constructs file path for a specific file in data folder\n",
    "    file_path = os.path.join('data',filename)\n",
    "    filenames.append(filename)\n",
    "    print(file_path)\n",
    "    \n",
    "    # Had errors reading certain files, so try different encodings\n",
    "    try:\n",
    "        #Use utf-8 encoding\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            content = file.read()\n",
    "    except UnicodeDecodeError:\n",
    "        #Try with a different encoding\n",
    "        with open(file_path, 'r', encoding='latin-1') as file:\n",
    "            content = file.read()\n",
    "        \n",
    "    #Convert to lowercase\n",
    "    content_lower = content.lower()\n",
    "    #Create tokens\n",
    "    tokens = word_tokenize(content_lower)\n",
    "    #Remove stop words \n",
    "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "    #Remove non-alphanumeric characters \n",
    "    processed_tokens = [re.sub(r'[^a-zA-Z0-9]+', '', word) for word in filtered_tokens]\n",
    "    #Remove singly occurring characters like 'm' or 'a'\n",
    "    processed_tokens = [word for word in processed_tokens if len(word) > 1]\n",
    "\n",
    "    #Add processed tokens to the set\n",
    "    all_words.update(processed_tokens)\n",
    "    processed_text = ' '.join(processed_tokens)\n",
    "\n",
    "    #Gets the file path to write the processed data\n",
    "    preprocessed_file_path = os.path.join(preprocessed_directory, filename)\n",
    "\n",
    "    #Write processed text to preprocessed_data\n",
    "    with open(preprocessed_file_path, 'w', encoding='utf-8') as file:\n",
    "        file.write(processed_text)\n",
    "        \n",
    "print(f\"Total unique words: {len(all_words)}\")\n",
    "print(\"Question 1 Completed\")\n",
    "\n",
    "#Initialize the inverted index as a dictionary\n",
    "inverted_index = {}\n",
    "#iterate through each word in all_words\n",
    "for word in all_words:\n",
    "    #assign an empty set for each word in the dictionary\n",
    "    inverted_index[word] = set()\n",
    "\n",
    "\n",
    "#Iterate through preprocessed files.\n",
    "for filename in os.listdir(preprocessed_directory):\n",
    "    file_path = os.path.join(preprocessed_directory, filename)\n",
    "    print(\"Adding file to inverted index: \", filename)\n",
    "    \n",
    "    with open(file_path, 'r', encoding = 'utf-8') as file:\n",
    "        content = file.read()\n",
    "        #Create a set of words in the document\n",
    "        words = set(content.split())\n",
    "        \n",
    "        #iterates through each word \n",
    "        for word in words:\n",
    "            #if word appears in dictionary, add filename to the corresponding set.\n",
    "            if word in inverted_index:\n",
    "                inverted_index[word].add(filename)\n",
    "\n",
    "# Write to a text file\n",
    "with open('inverted_index.txt', 'w') as file:\n",
    "    for key, values in inverted_index.items():\n",
    "        # Convert the set to a list and join with commas\n",
    "        values_list = ', '.join(values)\n",
    "        file.write(f\"{key}: {values_list}\\n\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unwanted docs: {'wisteria.txt'}\n",
      "QUERY #1\n",
      "\n",
      "Number of matched documents: 248\n",
      "Minimum number of comparisons required: 1\n",
      "Retrieved document names: ['fable.txt', 'reality.txt', 'alissadl.txt', 'shrdfarm.txt', 'hitch2.txt', 'graymare.txt', '7oldsamr.txt', 'foxngrap.txt', 'lrrhood.txt', 'fish.txt', 'blind.txt', 'advtthum.txt', 'hotline1.txt', '3student.txt', 'aminegg.txt', 'space.txt', 'gold3ber.txt', 'dakota.txt', 'parotsha.txt', 'greedog.txt', 'stairdre.txt', '100west.txt', 'aesopa10.txt', '6ablemen.txt', 'holmesbk.txt', 'campfire.txt', 'alad10.txt', 'partya.txt', 'frogp.txt', 'lionwar.txt', 'bluebrd.txt', 'arctic.txt', 'bulironb.txt', 'traitor.txt', 'gatherng.txt', 'aquith.txt', 'silverb.txt', 'tearglas.txt', 'assorted.txt', 'the-tree.txt', 'bgcspoof.txt', 'zombies.txt', 'fleas.txt', 'mcdonaldl.txt', 'lpeargrl.txt', 'tctac.txt', 'flktrp.txt', 'long1-3.txt', 'beautbst.txt', 'aislesix.txt', 'discocanbefun.txt', 'lmermaid.txt', 'musibrem.txt', 'psf.txt', 'tcoa.txt', 'dicegame.txt', 'wolflamb.txt', '13chil.txt', 'abbey.txt', 'roger1.txt', 'bgb.txt', 'startrek.txt', 'haretort.txt', 'mario.txt', 'charlie.txt', '6napolen.txt', 'foxncrow.txt', 'game.txt', 'wolfcran.txt', 'hareleph.txt', 'sre-dark.txt', '7voysinb.txt', 'goldenp.txt', 'rid.txt', 'pphamlin.txt', 'bulfelis.txt', 'running.txt', 'glimpse1.txt', 'kzap.txt', 'sight.txt', 'timetrav.txt', 'monkking.txt', 'jackbstl.txt', 'blh.txt', 'dicksong.txt', 'poem-4.txt', 'aircon.txt', 'tree.txt', 'terrorbears.txt', 'blabnove.txt', 'cabin.txt', 'empsjowk.txt', 'poplstrm.txt', '5orange.txt', 'aesop11.txt', 'encamp01.txt', 'hellmach.txt', 'oxfrog.txt', 'radar_ra.txt', 'lament.txt', 'sleprncs.txt', 'empty.txt', 'clevdonk.txt', 'blackp.txt', 'history5.txt', 'hound-b.txt', 'musgrave.txt', 'spiders.txt', 'bulolli2.txt', 'helmfuse.txt', 'sick-kid.txt', 'hitch3.txt', 'spider.txt', 'unluckwr.txt', 'gloves.txt', 'weaver.txt', 'mtinder.txt', 'advsayed.txt', 'fantasy.txt', 'weeprncs.txt', 'friends.txt', 'telefone.txt', 'hotline3.txt', 'empnclot.txt', 'bullove.txt', 'vaincrow.txt', 'dskool.txt', 'kharian.txt', 'bulnoopt.txt', 'uglyduck.txt', 'mydream.txt', 'beggars.txt', 'cardcnt.txt', 'deer.txt', 'horswolf.txt', 'mindprob.txt', 'hell4.txt', 'pinocch.txt', 'pussboot.txt', 'bulmrx.txt', 'ab40thv.txt', 'cybersla.txt', 'cmoutmou.txt', 'lgoldbrd.txt', 'snow.txt', 'missing.txt', 'monksol.txt', 'vgilante.txt', '3lpigs.txt', 'lmtchgrl.txt', 'sanpedr2.txt', 'dopedenn.txt', 'shoscomb.txt', 'tailbear.txt', 'mouslion.txt', '4moons.txt', 'retrib.txt', 'adler.txt', 'plescopm.txt', 'shulk.txt', 'bulphrek.txt', 'keepmodu.txt', 'bruce-p.txt', 'bulprint.txt', 'cooldark.txt', 'darkness.txt', 'ccm.txt', 'buggy.txt', 'foxnstrk.txt', 'bulhuntr.txt', 'kneeslapper.txt', 'vampword.txt', 'sucker.txt', 'poem-2.txt', 'rainda.txt', 'floobs.txt', 'abyss.txt', 'obstgoat.txt', 'sunday.txt', 'angry_ca.txt', 'bulzork1.txt', 'mattress.txt', 'paink-ws.txt', 'batlslau.txt', 'yukon.txt', 'buldream.txt', '3gables.txt', 'narciss.txt', 'goldfish.txt', 'fgoose.txt', 'fred.txt', 'wolf7kid.txt', 'bureau.txt', 'snowqn1.txt', 'veiledl.txt', 'adv_alad.txt', 'diaryflf.txt', 'taxnovel.txt', 'redragon.txt', 'melissa.txt', 'girlclub.txt', 'modemhippy.txt', 'goldgoos.txt', 'hareporc.txt', 'healer.txt', 'buldetal.txt', 'mazarin.txt', 'tinsoldr.txt', 'pepdegener.txt', 'lionmane.txt', 'solitary.txt', 'bulnland.txt', 'wlgirl.txt', 'bulolli1.txt', 'crabhern.txt', 'hansgrtl.txt', 'gemdra.txt', 'bishop00.txt', 'dtruck.txt', 'mike.txt', 'lure.txt', 'poem-1.txt', 'socialvikings.txt', 'antcrick.txt', 'omarsheh.txt', 'vainsong.txt', 'sqzply.txt', 'enginer.txt', '3wishes.txt', 'lionmosq.txt', 'bagelman.txt', 'emperor3.txt', 'gulliver.txt', 'snowmaid.txt', 'life.txt', 'horsdonk.txt', 'panama.txt', 'enya_trn.txt', 'knuckle.txt', 'excerpt.txt', 'pregn.txt', 'flute.txt', 'elveshoe.txt', 'hole2nar.txt', 'berternie.txt', 'hotline4.txt', 'flytrunk.txt', 'imonly17.txt']\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "N = int(input(\"Number of Queries: \"))\n",
    "\n",
    "for n in range(N):\n",
    "    while True:\n",
    "        input_s = input(\"Input sentence: \")\n",
    "        #INPUT FORMATTED AS op1,op2,... (i.e. OR,AND NOT)\n",
    "        input_o = input(\"Input operation sequence: \")\n",
    "        query_o = input_o.split(',')\n",
    "        \n",
    "        ## preprocess query sentence, similar to how tokens are preprocessed\n",
    "        input_s = input_s.lower()\n",
    "        input_s = word_tokenize(input_s)\n",
    "        q_tokens = [word for word in input_s if word not in stop_words]\n",
    "        q_tokens = [re.sub(r'[^a-zA-Z0-9]+', '', word) for word in q_tokens]\n",
    "        query_s = [word for word in q_tokens if len(word) > 1]\n",
    "        \n",
    "        if len(query_s) != len(query_o)+1:\n",
    "            print(\"ERROR: Unmatching number of tokens/ops, try again\")\n",
    "            print(\"Tokens: {}\".format(query_s))\n",
    "            print(\"Operations: {}\".format(query_o))\n",
    "            print(\"---\")\n",
    "            continue\n",
    "        elif not set(query_o).issubset(['AND','OR','OR NOT','AND NOT']):\n",
    "            print(\"ERROR: Invalid operator(s), try again\")\n",
    "            print(\"Tokens: {}\".format(query_s))\n",
    "            print(\"Operations: {}\".format(query_o))\n",
    "            print(\"---\")\n",
    "            continue\n",
    "        break\n",
    "    \n",
    "    doc_list = inverted_index[query_s[0]]\n",
    "    \n",
    "    i = 1\n",
    "    comp = 0\n",
    "    \n",
    "    while i < len(query_s):\n",
    "        word = query_s[i]\n",
    "        op = query_o[i-1]\n",
    "        \n",
    "        match op:\n",
    "            case 'AND':\n",
    "                # intersection with current doc_list\n",
    "                doc_list.intersection_update(inverted_index[word])\n",
    "                \n",
    "            case 'OR':\n",
    "                # add searched word doc_list to current list\n",
    "                doc_list.update(inverted_index[word])\n",
    "                \n",
    "            case 'AND NOT':\n",
    "                #get word document list, check if any in doc_list, delete from list\n",
    "                for d in list(inverted_index[word]):\n",
    "                    doc_list.discard(d)\n",
    "                    comp +=1 \n",
    "                \n",
    "            case 'OR NOT':\n",
    "                unwanted_docs = inverted_index[word]\n",
    "                doc_list = doc_list - unwanted_docs\n",
    "                #get every document which does not have current word\n",
    "                for fn in filenames:\n",
    "                    if fn not in doc_list and fn not in inverted_index[word]:\n",
    "                        doc_list.add(fn)\n",
    "                        comp +=1\n",
    "                #remove documents containing current word if they have already been added        \n",
    "                #unwanted_docs = inverted_index[word]\n",
    "                #print(\"unwanted docs:\", unwanted_docs)\n",
    "                #doc_list = doc_list - unwanted_docs\n",
    "                \n",
    "                \n",
    "               \n",
    "\n",
    "                \n",
    "        i+=1\n",
    "    print(\"QUERY #{}\\n\".format(n+1))\n",
    "    print(\"Number of matched documents: \" + str(len(doc_list)))\n",
    "    print(\"Minimum number of comparisons required: \" + str(comp))\n",
    "    print(\"Retrieved document names: \" + str(list(doc_list)))\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
