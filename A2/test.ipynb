{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Rida\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Rida\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as plt\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#Download nltk stuff\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "#Set up stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "preprocessed_directory = 'preprocessed_data'\n",
    "all_words = set()\n",
    "# list of all file names\n",
    "filenames = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positional index complete\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#initialize the positional index as a dictionary\n",
    "positional_index = {}\n",
    "term_count = {}\n",
    "term_total_max = {}\n",
    "\"\"\"{doc : [# of terms, term with max occ.]}\n",
    "    count all times each word appears within doc\n",
    "    temp = {word : count}\n",
    "    term_count = {doc : {word : count}}\n",
    "\"\"\"\n",
    "\n",
    "#Load all words from preprocessed files and build the positional index:\n",
    "for filename in os.listdir(preprocessed_directory):\n",
    "    file_path = os.path.join(preprocessed_directory,filename)\n",
    "    temp = {}\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "        words = content.split()\n",
    "        \n",
    "        #iterate through each word and its index:\n",
    "        for index, word in enumerate(words):\n",
    "            if word not in positional_index:\n",
    "                positional_index[word] = {}\n",
    "            if filename not in positional_index[word]:\n",
    "                positional_index[word][filename] = []\n",
    "                temp[word] = 0\n",
    "            positional_index[word][filename].append(index)\n",
    "            temp[word] += 1\n",
    "            \n",
    "        term_count[filename] = temp.copy()\n",
    "        m = max(temp, key=temp.get)\n",
    "        term_total_max[filename] = [len(words), temp[m], m]\n",
    "#position that the word occurs in is relative to the list of words and not the number of characters. So if document contains \"This is\", \"This\" is at position 0 and \"is\" at position 1\n",
    "            \n",
    "            \n",
    "# Write positional index to a file:\n",
    "with open('positional_index.txt', 'w') as file:\n",
    "    for word, documents in positional_index.items():\n",
    "        file.write(f\"{word}: \")\n",
    "        entries = []\n",
    "        for doc, positions in documents.items():\n",
    "            positions_str = ', '.join(map(str, positions))  # Convert list of positions to string\n",
    "            entries.append(f\"{doc} [{positions_str}]\")\n",
    "        document_positions = '; '.join(entries)  # Join all document entries with semicolon\n",
    "        file.write(f\"{document_positions}\\n\")\n",
    "        file.write(\"\\n\")\n",
    "\n",
    "print(\"Positional index complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dakota.txt: [15921, 15922]\n",
      "\n",
      "\n",
      "hitch2.txt: [23937, 23938]\n",
      "\n",
      "\n",
      "sucker.txt: [111, 112]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    phrase = input(\"Please enter a phrase:   \")\n",
    "    phrase = phrase.lower()\n",
    "\n",
    "    words = phrase.split()\n",
    "    if len(words) > 5:\n",
    "        print(\"Query length must be less than 5.\")\n",
    "    else:\n",
    "        break\n",
    "\n",
    "\n",
    "\n",
    "#Dictionary to store the combined document positions for the phrase.\n",
    "phrase_positions = {}\n",
    "\n",
    "\n",
    "#collect positions for each word in the phrase\n",
    "for word in words:\n",
    "    if word in positional_index:\n",
    "        for document, positions in positional_index[word].items():\n",
    "            if document not in phrase_positions:\n",
    "                phrase_positions[document] = []\n",
    "            phrase_positions[document].extend(positions)\n",
    "            \n",
    "#sorting the positions in each document\n",
    "for doc in phrase_positions:\n",
    "    phrase_positions[doc].sort()\n",
    "\n",
    "#print:\n",
    "#for doc, positions in phrase_positions.items():\n",
    "    #print(f\"{doc}: {positions}\")\n",
    "    \n",
    "results = {}\n",
    "\n",
    "# Check for sequences of consecutive numbers matching the phrase length\n",
    "for doc, positions in phrase_positions.items():\n",
    "    if len(positions) < len(words):\n",
    "        continue  # Skip if there aren't enough positions\n",
    "\n",
    "    # Search for consecutive positions\n",
    "    for i in range(len(positions) - len(words) + 1):\n",
    "        # Check if the next positions are consecutive\n",
    "        if all(positions[i + j] == positions[i] + j for j in range(len(words))):\n",
    "            if doc not in results:\n",
    "                results[doc] = []\n",
    "            results[doc].extend(positions[i:i+len(words)])  # Extend flat list\n",
    "\n",
    "# Output the results:\n",
    "for doc, pos_list in results.items():\n",
    "    print(f\"{doc}: {sorted(set(pos_list))}\")  # Remove duplicates and sort\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################\n",
    "# TF-IDF MATRIX\n",
    "\"\"\"\n",
    "Term Frequency\n",
    "Inverted Document Frequency\n",
    "\n",
    "Matrix = columns[[row],[row],[row]] <-- way to reduce storage size?\n",
    "\n",
    "\n",
    "same dataset as Q1\n",
    "tf_dict = {'this' : {'100west.txt' : count, ...}}\n",
    "for each word in dictionary:\n",
    "    for each document in dictionary:\n",
    "        dic = {}\n",
    "        \n",
    "        \n",
    "        \n",
    "\"\"\"\n",
    "import math\n",
    "DOC_TOTAL = 249\n",
    "\n",
    "#term_count = {}\n",
    "#doc_count = {}\n",
    "idf = {}\n",
    "\n",
    "for word in positional_index:\n",
    "    #d = {}\n",
    "    #for doc in positional_index[word]:\n",
    "    #    d.update({doc : len(positional_index[word][doc])})\n",
    "        \n",
    "    #term_count.update({word : d.copy()})\n",
    "    #doc_count.update({word : len(positional_index[word])})\n",
    "    idf[word] = math.log(DOC_TOTAL / (len(positional_index[word])+1))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "final solution:\n",
    "TF-IDF dataframe for each TF scheme (5 dataframes)\n",
    "\n",
    "need:\n",
    "total number of terms in each document: dict {doc : [# of terms, term with max occ]}\n",
    "term with most amount of occurence in each document: -------------->^^^^^\n",
    "\n",
    "create query vector => populate vector = vocab length with random values\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "matrix = pd.DataFrame(float(0), index=list(idf.keys()),columns=list(term_count.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    tf_variant = input(\"Select term freq variant: bin | rc | tf | ln | dn\")\n",
    "    if tf_variant not in ['bin','rc','tf','ln','dn']:\n",
    "        print(\"Invalid variant.\")\n",
    "    else:\n",
    "        break\n",
    "    \n",
    "\n",
    "    \n",
    "for doc, values in term_count.items():\n",
    "    for term, count in values.items():\n",
    "        match tf_variant:\n",
    "            case 'bin':\n",
    "                matrix.loc[term,doc] = idf[term]\n",
    "            case 'rc':\n",
    "                matrix.loc[term,doc] = count*idf[term]\n",
    "            case 'tf':\n",
    "                matrix.loc[term,doc] = (count/(term_total_max[doc][0]))*idf[term]\n",
    "            case 'ln':\n",
    "                matrix.loc[term,doc] = math.log(1+count)*idf[term]\n",
    "            case 'dn':\n",
    "                matrix.loc[term,doc] = (0.5 + 0.5*(count/term_total_max[doc][1]))*idf[term]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.random.seed = 3333\n",
    "#query_vector = np.random.uniform(0, 5, 44108)\n",
    "#query_vector = np.random.rand(44108)\n",
    "q = input(\"enter query: \")\n",
    "q = q.lower()\n",
    "q = word_tokenize(q)\n",
    "q = [word for word in q if word not in stop_words]\n",
    "q = [re.sub(r'[\\W_]+', '', word) for word in q if word]\n",
    "q = [word for word in q if len(word) > 1]\n",
    "\n",
    "query_count = {}\n",
    "\n",
    "for word in q:\n",
    "    if word not in query_count.keys():\n",
    "        query_count[word] = 1\n",
    "    else:\n",
    "        query_count[word] += 1\n",
    "\n",
    "query_vector = pd.Series(float(0), index=matrix.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word, count in query_count.items():\n",
    "    query_vector[word] = count\n",
    "    \"\"\"match tf_variant:\n",
    "        case 'bin':\n",
    "            query_vector[word] = idf[word]\n",
    "        case 'rc':\n",
    "            query_vector[word] = count*idf[word]\n",
    "        case 'tf':\n",
    "            query_vector[word] = (count/(term_total_max[doc][0]))*idf[word]\n",
    "        case 'ln':\n",
    "            query_vector[word] = math.log(1+count)*idf[word]\n",
    "        case 'dn':\n",
    "            query_vector[word] = (0.5 + (0.5* (count/term_total_max[doc][1])))*idf[word]\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank = pd.Series()\n",
    "rank_cos = pd.Series()\n",
    "\n",
    "for doc, arr in matrix.items():\n",
    "    rank[doc] = np.dot(query_vector,arr)\n",
    "    rank_cos[doc] = np.dot(query_vector,arr) / (np.linalg.norm(query_vector)*np.linalg.norm(arr))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sucker.txt      0.077948\n",
      "vampword.txt    0.019607\n",
      "empsjowk.txt    0.004563\n",
      "lmtchgrl.txt    0.004432\n",
      "narciss.txt     0.003949\n",
      "dtype: float64\n",
      "\n",
      "sucker.txt      0.421447\n",
      "vampword.txt    0.046678\n",
      "roger1.txt      0.044543\n",
      "running.txt     0.035433\n",
      "empsjowk.txt    0.034567\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(rank.sort_values(ascending=False).head())\n",
    "print()\n",
    "print(rank_cos.sort_values(ascending=False).head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nBINARY\\npros:\\ncons:\\n\\nRAW COUNT\\npros:\\ncons:\\n\\nTERM FREQUENCY\\n\\n\\n'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "BINARY\n",
    "pros:\n",
    "cons:\n",
    "\n",
    "RAW COUNT\n",
    "pros:\n",
    "cons:\n",
    "\n",
    "TERM FREQUENCY\n",
    "pros:\n",
    "cons:\n",
    "\n",
    "LOG NORMALIZATION\n",
    "pros:\n",
    "cons:\n",
    "\n",
    "DOUBLE NORMALIZATION\n",
    "pros:\n",
    "cons:\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
