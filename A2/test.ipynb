{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Rida\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Rida\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as plt\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#Download nltk stuff\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "#Set up stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "preprocessed_directory = 'preprocessed_data'\n",
    "all_words = set()\n",
    "# list of all file names\n",
    "filenames = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data\\100west.txt\n",
      "data\\13chil.txt\n",
      "data\\3gables.txt\n",
      "data\\3lpigs.txt\n",
      "data\\3student.txt\n",
      "data\\3wishes.txt\n",
      "data\\4moons.txt\n",
      "data\\5orange.txt\n",
      "data\\6ablemen.txt\n",
      "data\\6napolen.txt\n",
      "data\\7oldsamr.txt\n",
      "data\\7voysinb.txt\n",
      "data\\ab40thv.txt\n",
      "data\\abbey.txt\n",
      "data\\abyss.txt\n",
      "data\\adler.txt\n",
      "data\\advsayed.txt\n",
      "data\\advtthum.txt\n",
      "data\\adv_alad.txt\n",
      "data\\aesop11.txt\n",
      "data\\aesopa10.txt\n",
      "data\\aircon.txt\n",
      "data\\aislesix.txt\n",
      "data\\alad10.txt\n",
      "data\\alissadl.txt\n",
      "data\\aminegg.txt\n",
      "data\\angry_ca.txt\n",
      "data\\antcrick.txt\n",
      "data\\aquith.txt\n",
      "data\\arctic.txt\n",
      "data\\assorted.txt\n",
      "data\\bagelman.txt\n",
      "data\\batlslau.txt\n",
      "data\\beautbst.txt\n",
      "data\\beggars.txt\n",
      "data\\berternie.txt\n",
      "data\\bgb.txt\n",
      "data\\bgcspoof.txt\n",
      "data\\bishop00.txt\n",
      "data\\blabnove.txt\n",
      "data\\blackp.txt\n",
      "data\\blh.txt\n",
      "data\\blind.txt\n",
      "data\\bluebrd.txt\n",
      "data\\bruce-p.txt\n",
      "data\\buggy.txt\n",
      "data\\buldetal.txt\n",
      "data\\buldream.txt\n",
      "data\\bulfelis.txt\n",
      "data\\bulhuntr.txt\n",
      "data\\bulironb.txt\n",
      "data\\bullove.txt\n",
      "data\\bulmrx.txt\n",
      "data\\bulnland.txt\n",
      "data\\bulnoopt.txt\n",
      "data\\bulolli1.txt\n",
      "data\\bulolli2.txt\n",
      "data\\bulphrek.txt\n",
      "data\\bulprint.txt\n",
      "data\\bulzork1.txt\n",
      "data\\bureau.txt\n",
      "data\\cabin.txt\n",
      "data\\campfire.txt\n",
      "data\\cardcnt.txt\n",
      "data\\ccm.txt\n",
      "data\\charlie.txt\n",
      "data\\clevdonk.txt\n",
      "data\\cmoutmou.txt\n",
      "data\\cooldark.txt\n",
      "data\\crabhern.txt\n",
      "data\\cybersla.txt\n",
      "data\\dakota.txt\n",
      "data\\darkness.txt\n",
      "data\\deer.txt\n",
      "data\\diaryflf.txt\n",
      "data\\dicegame.txt\n",
      "data\\dicksong.txt\n",
      "data\\discocanbefun.txt\n",
      "data\\dopedenn.txt\n",
      "data\\dskool.txt\n",
      "data\\dtruck.txt\n",
      "data\\elveshoe.txt\n",
      "data\\emperor3.txt\n",
      "data\\empnclot.txt\n",
      "data\\empsjowk.txt\n",
      "data\\empty.txt\n",
      "data\\encamp01.txt\n",
      "data\\enginer.txt\n",
      "data\\enya_trn.txt\n",
      "data\\excerpt.txt\n",
      "data\\fable.txt\n",
      "data\\fantasy.txt\n",
      "data\\fgoose.txt\n",
      "data\\fish.txt\n",
      "data\\fleas.txt\n",
      "data\\flktrp.txt\n",
      "data\\floobs.txt\n",
      "data\\flute.txt\n",
      "data\\flytrunk.txt\n",
      "data\\foxncrow.txt\n",
      "data\\foxngrap.txt\n",
      "data\\foxnstrk.txt\n",
      "data\\fred.txt\n",
      "data\\friends.txt\n",
      "data\\frogp.txt\n",
      "data\\game.txt\n",
      "data\\gatherng.txt\n",
      "data\\gemdra.txt\n",
      "data\\girlclub.txt\n",
      "data\\glimpse1.txt\n",
      "data\\gloves.txt\n",
      "data\\gold3ber.txt\n",
      "data\\goldenp.txt\n",
      "data\\goldfish.txt\n",
      "data\\goldgoos.txt\n",
      "data\\graymare.txt\n",
      "data\\greedog.txt\n",
      "data\\gulliver.txt\n",
      "data\\hansgrtl.txt\n",
      "data\\hareleph.txt\n",
      "data\\hareporc.txt\n",
      "data\\haretort.txt\n",
      "data\\healer.txt\n",
      "data\\hell4.txt\n",
      "data\\hellmach.txt\n",
      "data\\helmfuse.txt\n",
      "data\\history5.txt\n",
      "data\\hitch2.txt\n",
      "data\\hitch3.txt\n",
      "data\\hole2nar.txt\n",
      "data\\holmesbk.txt\n",
      "data\\horsdonk.txt\n",
      "data\\horswolf.txt\n",
      "data\\hotline1.txt\n",
      "data\\hotline3.txt\n",
      "data\\hotline4.txt\n",
      "data\\hound-b.txt\n",
      "data\\imonly17.txt\n",
      "data\\jackbstl.txt\n",
      "data\\keepmodu.txt\n",
      "data\\kharian.txt\n",
      "data\\kneeslapper.txt\n",
      "data\\knuckle.txt\n",
      "data\\kzap.txt\n",
      "data\\lament.txt\n",
      "data\\lgoldbrd.txt\n",
      "data\\life.txt\n",
      "data\\lionmane.txt\n",
      "data\\lionmosq.txt\n",
      "data\\lionwar.txt\n",
      "data\\lmermaid.txt\n",
      "data\\lmtchgrl.txt\n",
      "data\\long1-3.txt\n",
      "data\\lpeargrl.txt\n",
      "data\\lrrhood.txt\n",
      "data\\lure.txt\n",
      "data\\mario.txt\n",
      "data\\mattress.txt\n",
      "data\\mazarin.txt\n",
      "data\\mcdonaldl.txt\n",
      "data\\melissa.txt\n",
      "data\\mike.txt\n",
      "data\\mindprob.txt\n",
      "data\\missing.txt\n",
      "data\\modemhippy.txt\n",
      "data\\monkking.txt\n",
      "data\\monksol.txt\n",
      "data\\mouslion.txt\n",
      "data\\mtinder.txt\n",
      "data\\musgrave.txt\n",
      "data\\musibrem.txt\n",
      "data\\mydream.txt\n",
      "data\\narciss.txt\n",
      "data\\obstgoat.txt\n",
      "data\\omarsheh.txt\n",
      "data\\oxfrog.txt\n",
      "data\\paink-ws.txt\n",
      "data\\panama.txt\n",
      "data\\parotsha.txt\n",
      "data\\partya.txt\n",
      "data\\pepdegener.txt\n",
      "data\\pinocch.txt\n",
      "data\\plescopm.txt\n",
      "data\\poem-1.txt\n",
      "data\\poem-2.txt\n",
      "data\\poem-4.txt\n",
      "data\\poplstrm.txt\n",
      "data\\pphamlin.txt\n",
      "data\\pregn.txt\n",
      "data\\psf.txt\n",
      "data\\pussboot.txt\n",
      "data\\radar_ra.txt\n",
      "data\\rainda.txt\n",
      "data\\reality.txt\n",
      "data\\redragon.txt\n",
      "data\\retrib.txt\n",
      "data\\rid.txt\n",
      "data\\roger1.txt\n",
      "data\\running.txt\n",
      "data\\sanpedr2.txt\n",
      "data\\shoscomb.txt\n",
      "data\\shrdfarm.txt\n",
      "data\\shulk.txt\n",
      "data\\sick-kid.txt\n",
      "data\\sight.txt\n",
      "data\\silverb.txt\n",
      "data\\sleprncs.txt\n",
      "data\\snow.txt\n",
      "data\\snowmaid.txt\n",
      "data\\snowqn1.txt\n",
      "data\\socialvikings.txt\n",
      "data\\solitary.txt\n",
      "data\\space.txt\n",
      "data\\spider.txt\n",
      "data\\spiders.txt\n",
      "data\\sqzply.txt\n",
      "data\\sre-dark.txt\n",
      "data\\stairdre.txt\n",
      "data\\startrek.txt\n",
      "data\\sucker.txt\n",
      "data\\sunday.txt\n",
      "data\\tailbear.txt\n",
      "data\\taxnovel.txt\n",
      "data\\tcoa.txt\n",
      "data\\tctac.txt\n",
      "data\\tearglas.txt\n",
      "data\\telefone.txt\n",
      "data\\terrorbears.txt\n",
      "data\\the-tree.txt\n",
      "data\\timetrav.txt\n",
      "data\\tinsoldr.txt\n",
      "data\\traitor.txt\n",
      "data\\tree.txt\n",
      "data\\uglyduck.txt\n",
      "data\\unluckwr.txt\n",
      "data\\vaincrow.txt\n",
      "data\\vainsong.txt\n",
      "data\\vampword.txt\n",
      "data\\veiledl.txt\n",
      "data\\vgilante.txt\n",
      "data\\weaver.txt\n",
      "data\\weeprncs.txt\n",
      "data\\wisteria.txt\n",
      "data\\wlgirl.txt\n",
      "data\\wolf7kid.txt\n",
      "data\\wolfcran.txt\n",
      "data\\wolflamb.txt\n",
      "data\\yukon.txt\n",
      "data\\zombies.txt\n"
     ]
    }
   ],
   "source": [
    "#Loop through each file in the directory\n",
    "doc_total = 0\n",
    "\n",
    "for filename in os.listdir('data'):\n",
    "    doc_total+=1\n",
    "    #Constructs file path for a specific file in data folder\n",
    "    file_path = os.path.join('data',filename)\n",
    "    filenames.append(filename)\n",
    "    print(file_path)\n",
    "    \n",
    "    # Had errors reading certain files, so try different encodings\n",
    "    try:\n",
    "        #Use utf-8 encoding\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            content = file.read()\n",
    "    except UnicodeDecodeError:\n",
    "        #Try with a different encoding\n",
    "        with open(file_path, 'r', encoding='latin-1') as file:\n",
    "            content = file.read()\n",
    "        \n",
    "    #Convert to lowercase\n",
    "    content_lower = content.lower()\n",
    "    #Create tokens\n",
    "    tokens = word_tokenize(content_lower)\n",
    "    #Remove stop words \n",
    "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "    #Remove punctuation \n",
    "    processed_tokens = [re.sub(r'[\\W_]+', '', word) for word in tokens if word]  # Remove punctuation\n",
    "    #Remove singly occurring characters like 'm' or 'a'\n",
    "    processed_tokens = [word for word in processed_tokens if len(word) > 1]\n",
    "\n",
    "    #Add processed tokens to the set\n",
    "    all_words.update(processed_tokens)\n",
    "    processed_text = ' '.join(processed_tokens)\n",
    "\n",
    "    #Gets the file path to write the processed data\n",
    "    preprocessed_file_path = os.path.join(preprocessed_directory, filename)\n",
    "\n",
    "    #Write processed text to preprocessed_data\n",
    "    with open(preprocessed_file_path, 'w', encoding='utf-8') as file:\n",
    "        file.write(processed_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique words: 44108\n",
      "Question 1 Completed\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total unique words: {len(all_words)}\")\n",
    "print(\"Question 1 Completed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positional index complete\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#initialize the positional index as a dictionary\n",
    "positional_index = {}\n",
    "\n",
    "#Load all words from preprocessed files and build the positional index:\n",
    "for filename in os.listdir(preprocessed_directory):\n",
    "    file_path = os.path.join(preprocessed_directory,filename)\n",
    "    \n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "        words = content.split()\n",
    "        \n",
    "        #iterate through each word and its index:\n",
    "        for index, word in enumerate(words):\n",
    "            if word not in positional_index:\n",
    "                positional_index[word] = {}\n",
    "            if filename not in positional_index[word]:\n",
    "                positional_index[word][filename] = []\n",
    "            positional_index[word][filename].append(index)\n",
    "#position that the word occurs in is relative to the list of words and not the number of characters. So if document contains \"This is\", \"This\" is at position 0 and \"is\" at position 1\n",
    "            \n",
    "            \n",
    "# Write positional index to a file:\n",
    "with open('positional_index.txt', 'w') as file:\n",
    "    for word, documents in positional_index.items():\n",
    "        file.write(f\"{word}: \")\n",
    "        entries = []\n",
    "        for doc, positions in documents.items():\n",
    "            positions_str = ', '.join(map(str, positions))  # Convert list of positions to string\n",
    "            entries.append(f\"{doc} [{positions_str}]\")\n",
    "        document_positions = '; '.join(entries)  # Join all document entries with semicolon\n",
    "        file.write(f\"{document_positions}\\n\")\n",
    "        file.write(\"\\n\")\n",
    "\n",
    "print(\"Positional index complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    phrase = input(\"Please enter a phrase:   \")\n",
    "    phrase = phrase.lower()\n",
    "\n",
    "    words = phrase.split()\n",
    "    if len(words) > 5:\n",
    "        print(\"Query length must be less than 5.\")\n",
    "    else:\n",
    "        break\n",
    "\n",
    "\n",
    "\n",
    "#Dictionary to store the combined document positions for the phrase.\n",
    "phrase_positions = {}\n",
    "\n",
    "\n",
    "#collect positions for each word in the phrase\n",
    "for word in words:\n",
    "    if word in positional_index:\n",
    "        for document, positions in positional_index[word].items():\n",
    "            if document not in phrase_positions:\n",
    "                phrase_positions[document] = []\n",
    "            phrase_positions[document].extend(positions)\n",
    "            \n",
    "#sorting the positions in each document\n",
    "for doc in phrase_positions:\n",
    "    phrase_positions[doc].sort()\n",
    "\n",
    "#print:\n",
    "#for doc, positions in phrase_positions.items():\n",
    "    #print(f\"{doc}: {positions}\")\n",
    "    \n",
    "results = {}\n",
    "\n",
    "# Check for sequences of consecutive numbers matching the phrase length\n",
    "for doc, positions in phrase_positions.items():\n",
    "    if len(positions) < len(words):\n",
    "        continue  # Skip if there aren't enough positions\n",
    "\n",
    "    # Search for consecutive positions\n",
    "    for i in range(len(positions) - len(words) + 1):\n",
    "        # Check if the next positions are consecutive\n",
    "        if all(positions[i + j] == positions[i] + j for j in range(len(words))):\n",
    "            if doc not in results:\n",
    "                results[doc] = []\n",
    "            results[doc].extend(positions[i:i+len(words)])  # Extend flat list\n",
    "\n",
    "# Output the results:\n",
    "for doc, pos_list in results.items():\n",
    "    print(f\"{doc}: {sorted(set(pos_list))}\")  # Remove duplicates and sort\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################\n",
    "# TF-IDF MATRIX\n",
    "\"\"\"\n",
    "Term Frequency\n",
    "Inverted Document Frequency\n",
    "\n",
    "Matrix = columns[[row],[row],[row]] <-- way to reduce storage size?\n",
    "\n",
    "\n",
    "same dataset as Q1\n",
    "tf_dict = {'this' : {'100west.txt' : count, ...}}\n",
    "for each word in dictionary:\n",
    "    for each document in dictionary:\n",
    "        dic = {}\n",
    "        \n",
    "        \n",
    "        \n",
    "\"\"\"\n",
    "tf_dic = {}\n",
    "doc_count = {}\n",
    "\n",
    "for word in positional_index:\n",
    "    d = {}\n",
    "    for doc in positional_index[word]:\n",
    "        d.update({doc : len(positional_index[word][doc])})\n",
    "        \n",
    "    tf_dic.update({word : d.copy()})\n",
    "    doc_count.update({word : len(positional_index[word])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf = {}\n",
    "idf['is'] = math.log(doc_total/(doc_count['is'] + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1468148683370448"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idf['is']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
