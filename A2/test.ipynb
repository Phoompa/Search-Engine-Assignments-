{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Rida\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Rida\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as plt\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#Download nltk stuff\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "#Set up stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "preprocessed_directory = 'preprocessed_data'\n",
    "all_words = set()\n",
    "# list of all file names\n",
    "filenames = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data\\100west.txt\n",
      "data\\13chil.txt\n",
      "data\\3gables.txt\n",
      "data\\3lpigs.txt\n",
      "data\\3student.txt\n",
      "data\\3wishes.txt\n",
      "data\\4moons.txt\n",
      "data\\5orange.txt\n",
      "data\\6ablemen.txt\n",
      "data\\6napolen.txt\n",
      "data\\7oldsamr.txt\n",
      "data\\7voysinb.txt\n",
      "data\\ab40thv.txt\n",
      "data\\abbey.txt\n",
      "data\\abyss.txt\n",
      "data\\adler.txt\n",
      "data\\advsayed.txt\n",
      "data\\advtthum.txt\n",
      "data\\adv_alad.txt\n",
      "data\\aesop11.txt\n",
      "data\\aesopa10.txt\n",
      "data\\aircon.txt\n",
      "data\\aislesix.txt\n",
      "data\\alad10.txt\n",
      "data\\alissadl.txt\n",
      "data\\aminegg.txt\n",
      "data\\angry_ca.txt\n",
      "data\\antcrick.txt\n",
      "data\\aquith.txt\n",
      "data\\arctic.txt\n",
      "data\\assorted.txt\n",
      "data\\bagelman.txt\n",
      "data\\batlslau.txt\n",
      "data\\beautbst.txt\n",
      "data\\beggars.txt\n",
      "data\\berternie.txt\n",
      "data\\bgb.txt\n",
      "data\\bgcspoof.txt\n",
      "data\\bishop00.txt\n",
      "data\\blabnove.txt\n",
      "data\\blackp.txt\n",
      "data\\blh.txt\n",
      "data\\blind.txt\n",
      "data\\bluebrd.txt\n",
      "data\\bruce-p.txt\n",
      "data\\buggy.txt\n",
      "data\\buldetal.txt\n",
      "data\\buldream.txt\n",
      "data\\bulfelis.txt\n",
      "data\\bulhuntr.txt\n",
      "data\\bulironb.txt\n",
      "data\\bullove.txt\n",
      "data\\bulmrx.txt\n",
      "data\\bulnland.txt\n",
      "data\\bulnoopt.txt\n",
      "data\\bulolli1.txt\n",
      "data\\bulolli2.txt\n",
      "data\\bulphrek.txt\n",
      "data\\bulprint.txt\n",
      "data\\bulzork1.txt\n",
      "data\\bureau.txt\n",
      "data\\cabin.txt\n",
      "data\\campfire.txt\n",
      "data\\cardcnt.txt\n",
      "data\\ccm.txt\n",
      "data\\charlie.txt\n",
      "data\\clevdonk.txt\n",
      "data\\cmoutmou.txt\n",
      "data\\cooldark.txt\n",
      "data\\crabhern.txt\n",
      "data\\cybersla.txt\n",
      "data\\dakota.txt\n",
      "data\\darkness.txt\n",
      "data\\deer.txt\n",
      "data\\diaryflf.txt\n",
      "data\\dicegame.txt\n",
      "data\\dicksong.txt\n",
      "data\\discocanbefun.txt\n",
      "data\\dopedenn.txt\n",
      "data\\dskool.txt\n",
      "data\\dtruck.txt\n",
      "data\\elveshoe.txt\n",
      "data\\emperor3.txt\n",
      "data\\empnclot.txt\n",
      "data\\empsjowk.txt\n",
      "data\\empty.txt\n",
      "data\\encamp01.txt\n",
      "data\\enginer.txt\n",
      "data\\enya_trn.txt\n",
      "data\\excerpt.txt\n",
      "data\\fable.txt\n",
      "data\\fantasy.txt\n",
      "data\\fgoose.txt\n",
      "data\\fish.txt\n",
      "data\\fleas.txt\n",
      "data\\flktrp.txt\n",
      "data\\floobs.txt\n",
      "data\\flute.txt\n",
      "data\\flytrunk.txt\n",
      "data\\foxncrow.txt\n",
      "data\\foxngrap.txt\n",
      "data\\foxnstrk.txt\n",
      "data\\fred.txt\n",
      "data\\friends.txt\n",
      "data\\frogp.txt\n",
      "data\\game.txt\n",
      "data\\gatherng.txt\n",
      "data\\gemdra.txt\n",
      "data\\girlclub.txt\n",
      "data\\glimpse1.txt\n",
      "data\\gloves.txt\n",
      "data\\gold3ber.txt\n",
      "data\\goldenp.txt\n",
      "data\\goldfish.txt\n",
      "data\\goldgoos.txt\n",
      "data\\graymare.txt\n",
      "data\\greedog.txt\n",
      "data\\gulliver.txt\n",
      "data\\hansgrtl.txt\n",
      "data\\hareleph.txt\n",
      "data\\hareporc.txt\n",
      "data\\haretort.txt\n",
      "data\\healer.txt\n",
      "data\\hell4.txt\n",
      "data\\hellmach.txt\n",
      "data\\helmfuse.txt\n",
      "data\\history5.txt\n",
      "data\\hitch2.txt\n",
      "data\\hitch3.txt\n",
      "data\\hole2nar.txt\n",
      "data\\holmesbk.txt\n",
      "data\\horsdonk.txt\n",
      "data\\horswolf.txt\n",
      "data\\hotline1.txt\n",
      "data\\hotline3.txt\n",
      "data\\hotline4.txt\n",
      "data\\hound-b.txt\n",
      "data\\imonly17.txt\n",
      "data\\jackbstl.txt\n",
      "data\\keepmodu.txt\n",
      "data\\kharian.txt\n",
      "data\\kneeslapper.txt\n",
      "data\\knuckle.txt\n",
      "data\\kzap.txt\n",
      "data\\lament.txt\n",
      "data\\lgoldbrd.txt\n",
      "data\\life.txt\n",
      "data\\lionmane.txt\n",
      "data\\lionmosq.txt\n",
      "data\\lionwar.txt\n",
      "data\\lmermaid.txt\n",
      "data\\lmtchgrl.txt\n",
      "data\\long1-3.txt\n",
      "data\\lpeargrl.txt\n",
      "data\\lrrhood.txt\n",
      "data\\lure.txt\n",
      "data\\mario.txt\n",
      "data\\mattress.txt\n",
      "data\\mazarin.txt\n",
      "data\\mcdonaldl.txt\n",
      "data\\melissa.txt\n",
      "data\\mike.txt\n",
      "data\\mindprob.txt\n",
      "data\\missing.txt\n",
      "data\\modemhippy.txt\n",
      "data\\monkking.txt\n",
      "data\\monksol.txt\n",
      "data\\mouslion.txt\n",
      "data\\mtinder.txt\n",
      "data\\musgrave.txt\n",
      "data\\musibrem.txt\n",
      "data\\mydream.txt\n",
      "data\\narciss.txt\n",
      "data\\obstgoat.txt\n",
      "data\\omarsheh.txt\n",
      "data\\oxfrog.txt\n",
      "data\\paink-ws.txt\n",
      "data\\panama.txt\n",
      "data\\parotsha.txt\n",
      "data\\partya.txt\n",
      "data\\pepdegener.txt\n",
      "data\\pinocch.txt\n",
      "data\\plescopm.txt\n",
      "data\\poem-1.txt\n",
      "data\\poem-2.txt\n",
      "data\\poem-4.txt\n",
      "data\\poplstrm.txt\n",
      "data\\pphamlin.txt\n",
      "data\\pregn.txt\n",
      "data\\psf.txt\n",
      "data\\pussboot.txt\n",
      "data\\radar_ra.txt\n",
      "data\\rainda.txt\n",
      "data\\reality.txt\n",
      "data\\redragon.txt\n",
      "data\\retrib.txt\n",
      "data\\rid.txt\n",
      "data\\roger1.txt\n",
      "data\\running.txt\n",
      "data\\sanpedr2.txt\n",
      "data\\shoscomb.txt\n",
      "data\\shrdfarm.txt\n",
      "data\\shulk.txt\n",
      "data\\sick-kid.txt\n",
      "data\\sight.txt\n",
      "data\\silverb.txt\n",
      "data\\sleprncs.txt\n",
      "data\\snow.txt\n",
      "data\\snowmaid.txt\n",
      "data\\snowqn1.txt\n",
      "data\\socialvikings.txt\n",
      "data\\solitary.txt\n",
      "data\\space.txt\n",
      "data\\spider.txt\n",
      "data\\spiders.txt\n",
      "data\\sqzply.txt\n",
      "data\\sre-dark.txt\n",
      "data\\stairdre.txt\n",
      "data\\startrek.txt\n",
      "data\\sucker.txt\n",
      "data\\sunday.txt\n",
      "data\\tailbear.txt\n",
      "data\\taxnovel.txt\n",
      "data\\tcoa.txt\n",
      "data\\tctac.txt\n",
      "data\\tearglas.txt\n",
      "data\\telefone.txt\n",
      "data\\terrorbears.txt\n",
      "data\\the-tree.txt\n",
      "data\\timetrav.txt\n",
      "data\\tinsoldr.txt\n",
      "data\\traitor.txt\n",
      "data\\tree.txt\n",
      "data\\uglyduck.txt\n",
      "data\\unluckwr.txt\n",
      "data\\vaincrow.txt\n",
      "data\\vainsong.txt\n",
      "data\\vampword.txt\n",
      "data\\veiledl.txt\n",
      "data\\vgilante.txt\n",
      "data\\weaver.txt\n",
      "data\\weeprncs.txt\n",
      "data\\wisteria.txt\n",
      "data\\wlgirl.txt\n",
      "data\\wolf7kid.txt\n",
      "data\\wolfcran.txt\n",
      "data\\wolflamb.txt\n",
      "data\\yukon.txt\n",
      "data\\zombies.txt\n"
     ]
    }
   ],
   "source": [
    "#Loop through each file in the directory\n",
    "\n",
    "for filename in os.listdir('data'):\n",
    "    #Constructs file path for a specific file in data folder\n",
    "    file_path = os.path.join('data',filename)\n",
    "    filenames.append(filename)\n",
    "    print(file_path)\n",
    "    \n",
    "    # Had errors reading certain files, so try different encodings\n",
    "    try:\n",
    "        #Use utf-8 encoding\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            content = file.read()\n",
    "    except UnicodeDecodeError:\n",
    "        #Try with a different encoding\n",
    "        with open(file_path, 'r', encoding='latin-1') as file:\n",
    "            content = file.read()\n",
    "        \n",
    "    #Convert to lowercase\n",
    "    content_lower = content.lower()\n",
    "    #Create tokens\n",
    "    tokens = word_tokenize(content_lower)\n",
    "    #Remove stop words \n",
    "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "    #Remove punctuation \n",
    "    processed_tokens = [re.sub(r'[\\W_]+', '', word) for word in tokens if word]  # Remove punctuation\n",
    "    #Remove singly occurring characters like 'm' or 'a'\n",
    "    processed_tokens = [word for word in processed_tokens if len(word) > 1]\n",
    "\n",
    "    #Add processed tokens to the set\n",
    "    all_words.update(processed_tokens)\n",
    "    processed_text = ' '.join(processed_tokens)\n",
    "\n",
    "    #Gets the file path to write the processed data\n",
    "    preprocessed_file_path = os.path.join(preprocessed_directory, filename)\n",
    "\n",
    "    #Write processed text to preprocessed_data\n",
    "    with open(preprocessed_file_path, 'w', encoding='utf-8') as file:\n",
    "        file.write(processed_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique words: 44108\n",
      "Question 1 Completed\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total unique words: {len(all_words)}\")\n",
    "print(\"Question 1 Completed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positional index complete\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#initialize the positional index as a dictionary\n",
    "positional_index = {}\n",
    "term_count = {}\n",
    "term_total_max = {}\n",
    "\"\"\"{doc : [# of terms, term with max occ.]}\n",
    "    count all times each word appears within doc\n",
    "    temp = {word : count}\n",
    "    term_count = {doc : {word : count}}\n",
    "\"\"\"\n",
    "\n",
    "#Load all words from preprocessed files and build the positional index:\n",
    "for filename in os.listdir(preprocessed_directory):\n",
    "    file_path = os.path.join(preprocessed_directory,filename)\n",
    "    temp = {}\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "        words = content.split()\n",
    "        \n",
    "        #iterate through each word and its index:\n",
    "        for index, word in enumerate(words):\n",
    "            if word not in positional_index:\n",
    "                positional_index[word] = {}\n",
    "            if filename not in positional_index[word]:\n",
    "                positional_index[word][filename] = []\n",
    "                temp[word] = 0\n",
    "            positional_index[word][filename].append(index)\n",
    "            temp[word] += 1\n",
    "            \n",
    "        term_count[filename] = temp.copy()\n",
    "        m = max(temp, key=temp.get)\n",
    "        term_total_max[filename] = [len(words), temp[m], m]\n",
    "#position that the word occurs in is relative to the list of words and not the number of characters. So if document contains \"This is\", \"This\" is at position 0 and \"is\" at position 1\n",
    "            \n",
    "            \n",
    "# Write positional index to a file:\n",
    "with open('positional_index.txt', 'w') as file:\n",
    "    for word, documents in positional_index.items():\n",
    "        file.write(f\"{word}: \")\n",
    "        entries = []\n",
    "        for doc, positions in documents.items():\n",
    "            positions_str = ', '.join(map(str, positions))  # Convert list of positions to string\n",
    "            entries.append(f\"{doc} [{positions_str}]\")\n",
    "        document_positions = '; '.join(entries)  # Join all document entries with semicolon\n",
    "        file.write(f\"{document_positions}\\n\")\n",
    "        file.write(\"\\n\")\n",
    "\n",
    "print(\"Positional index complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    phrase = input(\"Please enter a phrase:   \")\n",
    "    phrase = phrase.lower()\n",
    "\n",
    "    words = phrase.split()\n",
    "    if len(words) > 5:\n",
    "        print(\"Query length must be less than 5.\")\n",
    "    else:\n",
    "        break\n",
    "\n",
    "\n",
    "\n",
    "#Dictionary to store the combined document positions for the phrase.\n",
    "phrase_positions = {}\n",
    "\n",
    "\n",
    "#collect positions for each word in the phrase\n",
    "for word in words:\n",
    "    if word in positional_index:\n",
    "        for document, positions in positional_index[word].items():\n",
    "            if document not in phrase_positions:\n",
    "                phrase_positions[document] = []\n",
    "            phrase_positions[document].extend(positions)\n",
    "            \n",
    "#sorting the positions in each document\n",
    "for doc in phrase_positions:\n",
    "    phrase_positions[doc].sort()\n",
    "\n",
    "#print:\n",
    "#for doc, positions in phrase_positions.items():\n",
    "    #print(f\"{doc}: {positions}\")\n",
    "    \n",
    "results = {}\n",
    "\n",
    "# Check for sequences of consecutive numbers matching the phrase length\n",
    "for doc, positions in phrase_positions.items():\n",
    "    if len(positions) < len(words):\n",
    "        continue  # Skip if there aren't enough positions\n",
    "\n",
    "    # Search for consecutive positions\n",
    "    for i in range(len(positions) - len(words) + 1):\n",
    "        # Check if the next positions are consecutive\n",
    "        if all(positions[i + j] == positions[i] + j for j in range(len(words))):\n",
    "            if doc not in results:\n",
    "                results[doc] = []\n",
    "            results[doc].extend(positions[i:i+len(words)])  # Extend flat list\n",
    "\n",
    "# Output the results:\n",
    "for doc, pos_list in results.items():\n",
    "    print(f\"{doc}: {sorted(set(pos_list))}\")  # Remove duplicates and sort\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################\n",
    "# TF-IDF MATRIX\n",
    "\"\"\"\n",
    "Term Frequency\n",
    "Inverted Document Frequency\n",
    "\n",
    "Matrix = columns[[row],[row],[row]] <-- way to reduce storage size?\n",
    "\n",
    "\n",
    "same dataset as Q1\n",
    "tf_dict = {'this' : {'100west.txt' : count, ...}}\n",
    "for each word in dictionary:\n",
    "    for each document in dictionary:\n",
    "        dic = {}\n",
    "        \n",
    "        \n",
    "        \n",
    "\"\"\"\n",
    "import math\n",
    "DOC_TOTAL = 249\n",
    "\n",
    "tc = {}\n",
    "#doc_count = {}\n",
    "idf = {}\n",
    "\n",
    "for word in positional_index:\n",
    "    d = {}\n",
    "    for doc in positional_index[word]:\n",
    "        d.update({doc : len(positional_index[word][doc])})\n",
    "        \n",
    "    tc.update({word : d.copy()})\n",
    "    #doc_count.update({word : len(positional_index[word])})\n",
    "    idf[word] = math.log(DOC_TOTAL / (len(positional_index[word])+1))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#idf = {}\n",
    "#idf['is'] = math.log(DOC_TOTAL/(doc_count['is'] + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "final solution:\n",
    "TF-IDF dataframe for each TF scheme (5 dataframes)\n",
    "\n",
    "need:\n",
    "total number of terms in each document: dict {doc : [# of terms, term with max occ]}\n",
    "term with most amount of occurence in each document: -------------->^^^^^\n",
    "\n",
    "create query vector => populate vector = vocab length with random values\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "matrix_bin = pd.DataFrame(float(0), index=list(idf.keys()),columns=list(term_count.keys()))\n",
    "matrix_rc = pd.DataFrame(float(0), index=list(idf.keys()),columns=list(term_count.keys()))\n",
    "matrix_tf = pd.DataFrame(float(0), index=list(idf.keys()),columns=list(term_count.keys()))\n",
    "matrix_ln = pd.DataFrame(float(0), index=list(idf.keys()),columns=list(term_count.keys()))\n",
    "matrix_dn = pd.DataFrame(float(0), index=list(idf.keys()),columns=list(term_count.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_count ## {doc : {term : count}} <-- may change to {term : {doc : count}}\n",
    "term_total_max ## {doc : [length, max term count, max term]}\n",
    "idf ## {term : score}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc, values in term_count.items():\n",
    "    for term, count in values.items():\n",
    "        matrix_bin.loc[term,doc] = idf[term]\n",
    "        matrix_rc.loc[term,doc] = count*idf[term]\n",
    "        matrix_tf.loc[term,doc] = (count/(term_total_max[doc][0]))*idf[term]\n",
    "        matrix_ln.loc[term,doc] = math.log(1+count)*idf[term]\n",
    "        matrix_dn.loc[term,doc] = 0.5 + (0.5* (count/term_total_max[doc][1]))*idf[term]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "for term, values in tc.items():\n",
    "    for doc, count in values.items():\n",
    "        matrix_bin.loc[term,doc] = idf[term]\n",
    "        matrix_rc.loc[term,doc] = count*idf[term]\n",
    "        matrix_tf.loc[term,doc] = (count/(term_total_max[doc][0]))*idf[term]\n",
    "        matrix_ln.loc[term,doc] = math.log(1+count)*idf[term]\n",
    "        matrix_dn.loc[term,doc] = 0.5 + (0.5* (count/term_total_max[doc][1]))*idf[term]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>100west.txt</th>\n",
       "      <th>13chil.txt</th>\n",
       "      <th>3gables.txt</th>\n",
       "      <th>3lpigs.txt</th>\n",
       "      <th>3student.txt</th>\n",
       "      <th>3wishes.txt</th>\n",
       "      <th>4moons.txt</th>\n",
       "      <th>5orange.txt</th>\n",
       "      <th>6ablemen.txt</th>\n",
       "      <th>6napolen.txt</th>\n",
       "      <th>...</th>\n",
       "      <th>vgilante.txt</th>\n",
       "      <th>weaver.txt</th>\n",
       "      <th>weeprncs.txt</th>\n",
       "      <th>wisteria.txt</th>\n",
       "      <th>wlgirl.txt</th>\n",
       "      <th>wolf7kid.txt</th>\n",
       "      <th>wolfcran.txt</th>\n",
       "      <th>wolflamb.txt</th>\n",
       "      <th>yukon.txt</th>\n",
       "      <th>zombies.txt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>this</th>\n",
       "      <td>0.501137</td>\n",
       "      <td>0.502927</td>\n",
       "      <td>0.507884</td>\n",
       "      <td>0.501672</td>\n",
       "      <td>0.506201</td>\n",
       "      <td>0.505996</td>\n",
       "      <td>0.503206</td>\n",
       "      <td>0.503708</td>\n",
       "      <td>0.50111</td>\n",
       "      <td>0.503758</td>\n",
       "      <td>...</td>\n",
       "      <td>0.503421</td>\n",
       "      <td>0.503854</td>\n",
       "      <td>0.503801</td>\n",
       "      <td>0.505261</td>\n",
       "      <td>0.500680</td>\n",
       "      <td>0.501445</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.502434</td>\n",
       "      <td>0.501171</td>\n",
       "      <td>0.502554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is</th>\n",
       "      <td>0.511432</td>\n",
       "      <td>0.504646</td>\n",
       "      <td>0.520298</td>\n",
       "      <td>0.500884</td>\n",
       "      <td>0.515584</td>\n",
       "      <td>0.501359</td>\n",
       "      <td>0.505088</td>\n",
       "      <td>0.510841</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.512235</td>\n",
       "      <td>...</td>\n",
       "      <td>0.505493</td>\n",
       "      <td>0.513764</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.515205</td>\n",
       "      <td>0.507017</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.503864</td>\n",
       "      <td>0.507898</td>\n",
       "      <td>0.501753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>shareware</th>\n",
       "      <td>0.508465</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trial</th>\n",
       "      <td>0.505167</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500155</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>project</th>\n",
       "      <td>0.505500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.505433</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>priveledge</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.503600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freshest</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.503600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wellmaintained</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.503600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sentenial</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.503600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>caryns</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.503600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>44108 rows × 249 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                100west.txt  13chil.txt  3gables.txt  3lpigs.txt  \\\n",
       "this               0.501137    0.502927     0.507884    0.501672   \n",
       "is                 0.511432    0.504646     0.520298    0.500884   \n",
       "shareware          0.508465    0.000000     0.000000    0.000000   \n",
       "trial              0.505167    0.000000     0.000000    0.000000   \n",
       "project            0.505500    0.000000     0.000000    0.000000   \n",
       "...                     ...         ...          ...         ...   \n",
       "priveledge         0.000000    0.000000     0.000000    0.000000   \n",
       "freshest           0.000000    0.000000     0.000000    0.000000   \n",
       "wellmaintained     0.000000    0.000000     0.000000    0.000000   \n",
       "sentenial          0.000000    0.000000     0.000000    0.000000   \n",
       "caryns             0.000000    0.000000     0.000000    0.000000   \n",
       "\n",
       "                3student.txt  3wishes.txt  4moons.txt  5orange.txt  \\\n",
       "this                0.506201     0.505996    0.503206     0.503708   \n",
       "is                  0.515584     0.501359    0.505088     0.510841   \n",
       "shareware           0.000000     0.000000    0.000000     0.000000   \n",
       "trial               0.000000     0.000000    0.000000     0.000000   \n",
       "project             0.000000     0.000000    0.000000     0.000000   \n",
       "...                      ...          ...         ...          ...   \n",
       "priveledge          0.000000     0.000000    0.000000     0.000000   \n",
       "freshest            0.000000     0.000000    0.000000     0.000000   \n",
       "wellmaintained      0.000000     0.000000    0.000000     0.000000   \n",
       "sentenial           0.000000     0.000000    0.000000     0.000000   \n",
       "caryns              0.000000     0.000000    0.000000     0.000000   \n",
       "\n",
       "                6ablemen.txt  6napolen.txt  ...  vgilante.txt  weaver.txt  \\\n",
       "this                 0.50111      0.503758  ...      0.503421    0.503854   \n",
       "is                   0.00000      0.512235  ...      0.505493    0.513764   \n",
       "shareware            0.00000      0.000000  ...      0.000000    0.000000   \n",
       "trial                0.00000      0.000000  ...      0.500155    0.000000   \n",
       "project              0.00000      0.000000  ...      0.505433    0.000000   \n",
       "...                      ...           ...  ...           ...         ...   \n",
       "priveledge           0.00000      0.000000  ...      0.000000    0.000000   \n",
       "freshest             0.00000      0.000000  ...      0.000000    0.000000   \n",
       "wellmaintained       0.00000      0.000000  ...      0.000000    0.000000   \n",
       "sentenial            0.00000      0.000000  ...      0.000000    0.000000   \n",
       "caryns               0.00000      0.000000  ...      0.000000    0.000000   \n",
       "\n",
       "                weeprncs.txt  wisteria.txt  wlgirl.txt  wolf7kid.txt  \\\n",
       "this                0.503801      0.505261    0.500680      0.501445   \n",
       "is                  0.000000      0.515205    0.507017      0.000000   \n",
       "shareware           0.000000      0.000000    0.000000      0.000000   \n",
       "trial               0.000000      0.000000    0.000000      0.000000   \n",
       "project             0.000000      0.000000    0.000000      0.000000   \n",
       "...                      ...           ...         ...           ...   \n",
       "priveledge          0.000000      0.000000    0.000000      0.000000   \n",
       "freshest            0.000000      0.000000    0.000000      0.000000   \n",
       "wellmaintained      0.000000      0.000000    0.000000      0.000000   \n",
       "sentenial           0.000000      0.000000    0.000000      0.000000   \n",
       "caryns              0.000000      0.000000    0.000000      0.000000   \n",
       "\n",
       "                wolfcran.txt  wolflamb.txt  yukon.txt  zombies.txt  \n",
       "this                     0.0      0.502434   0.501171     0.502554  \n",
       "is                       0.0      0.503864   0.507898     0.501753  \n",
       "shareware                0.0      0.000000   0.000000     0.000000  \n",
       "trial                    0.0      0.000000   0.000000     0.000000  \n",
       "project                  0.0      0.000000   0.000000     0.000000  \n",
       "...                      ...           ...        ...          ...  \n",
       "priveledge               0.0      0.000000   0.000000     0.503600  \n",
       "freshest                 0.0      0.000000   0.000000     0.503600  \n",
       "wellmaintained           0.0      0.000000   0.000000     0.503600  \n",
       "sentenial                0.0      0.000000   0.000000     0.503600  \n",
       "caryns                   0.0      0.000000   0.000000     0.503600  \n",
       "\n",
       "[44108 rows x 249 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix_dn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
