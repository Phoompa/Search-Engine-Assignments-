{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Rida\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Rida\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as plt\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#Download nltk stuff\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "#Set up stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "preprocessed_directory = 'preprocessed_data'\n",
    "all_words = set()\n",
    "# list of all file names\n",
    "filenames = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loop through each file in the directory\n",
    "\n",
    "for filename in os.listdir('data'):\n",
    "    #Constructs file path for a specific file in data folder\n",
    "    file_path = os.path.join('data',filename)\n",
    "    filenames.append(filename)\n",
    "    print(file_path)\n",
    "    \n",
    "    # Had errors reading certain files, so try different encodings\n",
    "    try:\n",
    "        #Use utf-8 encoding\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            content = file.read()\n",
    "    except UnicodeDecodeError:\n",
    "        #Try with a different encoding\n",
    "        with open(file_path, 'r', encoding='latin-1') as file:\n",
    "            content = file.read()\n",
    "        \n",
    "    #Convert to lowercase\n",
    "    content_lower = content.lower()\n",
    "    #Create tokens\n",
    "    tokens = word_tokenize(content_lower)\n",
    "    #Remove stop words \n",
    "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "    #Remove punctuation \n",
    "    processed_tokens = [re.sub(r'[\\W_]+', '', word) for word in tokens if word]  # Remove punctuation\n",
    "    #Remove singly occurring characters like 'm' or 'a'\n",
    "    processed_tokens = [word for word in processed_tokens if len(word) > 1]\n",
    "\n",
    "    #Add processed tokens to the set\n",
    "    all_words.update(processed_tokens)\n",
    "    processed_text = ' '.join(processed_tokens)\n",
    "\n",
    "    #Gets the file path to write the processed data\n",
    "    preprocessed_file_path = os.path.join(preprocessed_directory, filename)\n",
    "\n",
    "    #Write processed text to preprocessed_data\n",
    "    with open(preprocessed_file_path, 'w', encoding='utf-8') as file:\n",
    "        file.write(processed_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique words: 44108\n",
      "Question 1 Completed\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total unique words: {len(all_words)}\")\n",
    "print(\"Question 1 Completed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positional index complete\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#initialize the positional index as a dictionary\n",
    "positional_index = {}\n",
    "term_count = {}\n",
    "term_total_max = {}\n",
    "\"\"\"{doc : [# of terms, term with max occ.]}\n",
    "    count all times each word appears within doc\n",
    "    temp = {word : count}\n",
    "    term_count = {doc : {word : count}}\n",
    "\"\"\"\n",
    "\n",
    "#Load all words from preprocessed files and build the positional index:\n",
    "for filename in os.listdir(preprocessed_directory):\n",
    "    file_path = os.path.join(preprocessed_directory,filename)\n",
    "    temp = {}\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "        words = content.split()\n",
    "        \n",
    "        #iterate through each word and its index:\n",
    "        for index, word in enumerate(words):\n",
    "            if word not in positional_index:\n",
    "                positional_index[word] = {}\n",
    "            if filename not in positional_index[word]:\n",
    "                positional_index[word][filename] = []\n",
    "                temp[word] = 0\n",
    "            positional_index[word][filename].append(index)\n",
    "            temp[word] += 1\n",
    "            \n",
    "        term_count[filename] = temp.copy()\n",
    "        m = max(temp, key=temp.get)\n",
    "        term_total_max[filename] = [len(words), temp[m], m]\n",
    "#position that the word occurs in is relative to the list of words and not the number of characters. So if document contains \"This is\", \"This\" is at position 0 and \"is\" at position 1\n",
    "            \n",
    "            \n",
    "# Write positional index to a file:\n",
    "with open('positional_index.txt', 'w') as file:\n",
    "    for word, documents in positional_index.items():\n",
    "        file.write(f\"{word}: \")\n",
    "        entries = []\n",
    "        for doc, positions in documents.items():\n",
    "            positions_str = ', '.join(map(str, positions))  # Convert list of positions to string\n",
    "            entries.append(f\"{doc} [{positions_str}]\")\n",
    "        document_positions = '; '.join(entries)  # Join all document entries with semicolon\n",
    "        file.write(f\"{document_positions}\\n\")\n",
    "        file.write(\"\\n\")\n",
    "\n",
    "print(\"Positional index complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cybersla.txt: [32228, 32229, 32230]\n",
      "\n",
      "\n",
      "dakota.txt: [18843, 18844, 18845]\n",
      "\n",
      "\n",
      "yukon.txt: [611, 612, 613, 6952, 6953, 6954, 11236, 11237, 11238]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    phrase = input(\"Please enter a phrase:   \")\n",
    "    phrase = phrase.lower()\n",
    "\n",
    "    words = phrase.split()\n",
    "    if len(words) > 5:\n",
    "        print(\"Query length must be less than 5.\")\n",
    "    else:\n",
    "        break\n",
    "\n",
    "\n",
    "\n",
    "#Dictionary to store the combined document positions for the phrase.\n",
    "phrase_positions = {}\n",
    "\n",
    "\n",
    "#collect positions for each word in the phrase\n",
    "for word in words:\n",
    "    if word in positional_index:\n",
    "        for document, positions in positional_index[word].items():\n",
    "            if document not in phrase_positions:\n",
    "                phrase_positions[document] = []\n",
    "            phrase_positions[document].extend(positions)\n",
    "            \n",
    "#sorting the positions in each document\n",
    "for doc in phrase_positions:\n",
    "    phrase_positions[doc].sort()\n",
    "\n",
    "#print:\n",
    "#for doc, positions in phrase_positions.items():\n",
    "    #print(f\"{doc}: {positions}\")\n",
    "    \n",
    "results = {}\n",
    "\n",
    "# Check for sequences of consecutive numbers matching the phrase length\n",
    "for doc, positions in phrase_positions.items():\n",
    "    if len(positions) < len(words):\n",
    "        continue  # Skip if there aren't enough positions\n",
    "\n",
    "    # Search for consecutive positions\n",
    "    for i in range(len(positions) - len(words) + 1):\n",
    "        # Check if the next positions are consecutive\n",
    "        if all(positions[i + j] == positions[i] + j for j in range(len(words))):\n",
    "            if doc not in results:\n",
    "                results[doc] = []\n",
    "            results[doc].extend(positions[i:i+len(words)])  # Extend flat list\n",
    "\n",
    "# Output the results:\n",
    "for doc, pos_list in results.items():\n",
    "    print(f\"{doc}: {sorted(set(pos_list))}\")  # Remove duplicates and sort\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################\n",
    "# TF-IDF MATRIX\n",
    "\"\"\"\n",
    "Term Frequency\n",
    "Inverted Document Frequency\n",
    "\n",
    "Matrix = columns[[row],[row],[row]] <-- way to reduce storage size?\n",
    "\n",
    "\n",
    "same dataset as Q1\n",
    "tf_dict = {'this' : {'100west.txt' : count, ...}}\n",
    "for each word in dictionary:\n",
    "    for each document in dictionary:\n",
    "        dic = {}\n",
    "        \n",
    "        \n",
    "        \n",
    "\"\"\"\n",
    "import math\n",
    "DOC_TOTAL = 249\n",
    "\n",
    "#term_count = {}\n",
    "#doc_count = {}\n",
    "idf = {}\n",
    "\n",
    "for word in positional_index:\n",
    "    #d = {}\n",
    "    #for doc in positional_index[word]:\n",
    "    #    d.update({doc : len(positional_index[word][doc])})\n",
    "        \n",
    "    #term_count.update({word : d.copy()})\n",
    "    #doc_count.update({word : len(positional_index[word])})\n",
    "    idf[word] = math.log(DOC_TOTAL / (len(positional_index[word])+1))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#idf = {}\n",
    "#idf['is'] = math.log(DOC_TOTAL/(doc_count['is'] + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "final solution:\n",
    "TF-IDF dataframe for each TF scheme (5 dataframes)\n",
    "\n",
    "need:\n",
    "total number of terms in each document: dict {doc : [# of terms, term with max occ]}\n",
    "term with most amount of occurence in each document: -------------->^^^^^\n",
    "\n",
    "create query vector => populate vector = vocab length with random values\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "matrix = pd.DataFrame(float(0), index=list(idf.keys()),columns=list(term_count.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_count ## {term : {doc : count}}\n",
    "term_total_max ## {doc : [length, max term count, max term]}\n",
    "idf ## {term : score}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    tf_variant = input(\"Select term freq variant: bin | rc | tf | ln | dn\")\n",
    "    if tf_variant not in ['bin','rc','tf','ln','dn']:\n",
    "        print(\"Invalid variant.\")\n",
    "    else:\n",
    "        break\n",
    "    \n",
    "\n",
    "    \n",
    "for doc, values in term_count.items():\n",
    "    for term, count in values.items():\n",
    "        match tf_variant:\n",
    "            case 'bin':\n",
    "                matrix.loc[term,doc] = idf[term]\n",
    "            case 'rc':\n",
    "                matrix.loc[term,doc] = count*idf[term]\n",
    "            case 'tf':\n",
    "                matrix.loc[term,doc] = (count/(term_total_max[doc][0]))*idf[term]\n",
    "            case 'ln':\n",
    "                matrix.loc[term,doc] = math.log(1+count)*idf[term]\n",
    "            case 'dn':\n",
    "                matrix.loc[term,doc] = 0.5 + (0.5* (count/term_total_max[doc][1]))*idf[term]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.random.seed = 3333\n",
    "#query_vector = np.random.uniform(0, 5, 44108)\n",
    "#query_vector = np.random.rand(44108)\n",
    "q = input(\"enter query: \")\n",
    "q = q.lower()\n",
    "q = q.split()\n",
    "\n",
    "query_count = {}\n",
    "\n",
    "for word in q:\n",
    "    if word not in query_count.keys():\n",
    "        query_count[word] = 1\n",
    "    else:\n",
    "        query_count[word] += 1\n",
    "\n",
    "query_vector = pd.Series(float(0), index=matrix.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word, count in query_count.items():\n",
    "    match tf_variant:\n",
    "        case 'bin':\n",
    "            query_vector[word] = 1\n",
    "        case 'rc':\n",
    "            query_vector[word] = count\n",
    "        case 'tf':\n",
    "            query_vector[word] = (count/(term_total_max[doc][0]))\n",
    "        case 'ln':\n",
    "            query_vector[word] = math.log(1+count)\n",
    "        case 'dn':\n",
    "            query_vector[word] = 0.5 + (0.5* (count/term_total_max[doc][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank = pd.Series()\n",
    "\n",
    "for doc, arr in matrix.items():\n",
    "    rank[doc] = np.dot(query_vector,arr)\n",
    "    #rank[doc] = np.linalg.norm(query_vector - arr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sucker.txt      0.979788\n",
       "deer.txt        0.769944\n",
       "bullove.txt     0.764239\n",
       "bulolli1.txt    0.763146\n",
       "running.txt     0.763094\n",
       "dtype: float64"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rank.sort_values(ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
